<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Shiqi Chen</title> <meta name="author" content="Shiqi Chen"> <meta name="description" content=""> <meta name="keywords" content="computational photography, optics, deep-learning, artificial intelligence"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%93%B8&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://tangeego.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/talks/">talks</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Shiqi</span> Chen </h1> <p class="desc">/</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/prof_pic-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/prof_pic-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/prof_pic-1400.webp"></source> <img src="/assets/img/prof_pic.jpg?08c4bf775c33e41346d25df7eeccbd62" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="prof_pic.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="clearfix"> <p>State Key Laboratory of Extreme Photonics and Instrumentation, Zhejiang Univerity, Hangzhou<br> chensq0120 [at] gmail.com, chenshiqi [at] zju.edu.cn<br> <a href="https://scholar.google.com/citations?user=gJCsz90AAAAJ&amp;hl" rel="external nofollow noopener" target="_blank">Google scholar</a> | <a href="https://github.com/TanGeeGo" rel="external nofollow noopener" target="_blank">Github</a> || <a href="https://www.zhihu.com/people/chenshiqi0120" rel="external nofollow noopener" target="_blank">Zhihu</a> | <a href="http://tangeego.github.io//assets/img/wechat_public_account.jpg" rel="external nofollow noopener" target="_blank">Wechat</a> | <a href="https://space.bilibili.com/318625627?spm_id_from=333.1007.0.0" rel="external nofollow noopener" target="_blank">Bilibili</a></p> <p>I am a Postdoctral Researcher in the <a href="https://www.ee.ucla.edu" rel="external nofollow noopener" target="_blank">Samueli ECE</a> at <a href="https://www.ucla.edu/" rel="external nofollow noopener" target="_blank">UCLA</a>, working with Prof. <a href="https://research.seas.ucla.edu/ozcan/" rel="external nofollow noopener" target="_blank">Aydogan Ozcan</a>.</p> <p>Previously, I received my Ph.D. at Zhejiang University, where I co-advised by Prof. <a href="https://person.zju.edu.cn/0086127" rel="external nofollow noopener" target="_blank">Huajun Feng</a> and Prof. <a href="https://person.zju.edu.cn/0089108#0" rel="external nofollow noopener" target="_blank">Zhihai Xu</a>. My PhD research focused on involving both applied optics and computer vision to realize sharper computational photography. I have extensive connections with the industry and some of my works has been implemented into the latest mobile terminals.</p> <p>Besides my name, you can also call me TanGeeGo, which is a nicky-ID through my grown-up (I won’t tell you the origin of TanGeeGo is that I couldn’t understand the english, “Time To GO!”, spoken by the <a href="https://wowpedia.fandom.com/wiki/Rifleman" rel="external nofollow noopener" target="_blank">Rifleman</a> when I was a child <img class="emoji" title=":satisfied:" alt=":satisfied:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f606.png" height="20" width="20">). I like anything that brings happiness, not only for me, but for all around. I believe science and technology are supposed to be fun after all.</p> <p><strong>Research interest:</strong> Optics, Computational Photography, Deep Learning, Low-level Computer Vision, and Advanced Imaging Devices. <em>Interested in collaboration? Free to Contact me.</em></p> </div> <h2><a href="/news/" style="color: inherit;">news</a></h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Jan 7, 2024</th> <td> One paper on jitter imaging simulation and restoration is accepted by AAAI, code is released <a href="https://github.com/JHW2000/JARNet" rel="external nofollow noopener" target="_blank">here</a> <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">Dec 10, 2023</th> <td> Two paper accepted by OSA Optics Express, one presents a paraxial zooming system, another presents a low-light enhancement algorithm <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">Sep 8, 2023</th> <td> My code about differentiable ray tracing and coherent superposition code is released, please check <a href="https://github.com/TanGeeGo/ImagingSimulation/tree/master/PSF_generation/ray_tracing" rel="external nofollow noopener" target="_blank">here</a>. Hope you enjoy! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">Aug 1, 2023</th> <td> One paper accepted by IEEE Transactions on Geoscience and Remote Sensing (TGRS), code and data is on the way! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">Jul 30, 2023</th> <td> Diliverd a talk in the “Future Scientists Forum” of Chinese Optical Society Academic Conference, Wuhan, China. (slides will update soonly) </td> </tr> </table> </div> </div> <h2><a href="/blog/" style="color: inherit;">latest posts</a></h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Jul 12, 2023</th> <td> <a class="news-title" href="/blog/2023/post-bibliography/">a post with bibliography</a> </td> </tr> <tr> <th scope="row">Jul 4, 2023</th> <td> <a class="news-title" href="/blog/2023/jupyter-notebook/">a post with jupyter notebook</a> </td> </tr> <tr> <th scope="row">May 12, 2023</th> <td> <a class="news-title" href="/blog/2023/custom-blockquotes/">a post with custom blockquotes</a> </td> </tr> </table> </div> </div> <h2><a href="/publications/" style="color: inherit;">selected publications</a></h2> <div class="publications"> <h2 class="bibliography">2023</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/chen_tpami-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/chen_tpami-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/chen_tpami-1400.webp"></source> <img src="/assets/img/publication_preview/chen_tpami.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="chen_tpami.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Chen_2022_TPAMI" class="col-sm-8"> <div class="title">Computational Optics for Mobile Terminals in Mass Production</div> <div class="author"> <em>Shiqi Chen</em>, Ting Lin, Huajun Feng, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Zhihai Xu, Qi Li, Yueting Chen' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, Apr 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/%5Btpami%5D%20Computational%20Optics%20for%20Mobile%20Terminals%20in%20Mass%20Production.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Correcting the optical aberrations and the manufacturing deviations of cameras is a challenging task. Due to the limitation on volume and the demand for mass production, existing mobile terminals cannot rectify optical degradation. In this work, we systematically construct the perturbed lens system model to illustrate the relationship between the deviated system parameters and the spatial frequency response (SFR) measured from photographs. To further address this issue, an optimization framework is proposed based on this model to build proxy cameras from the machining samples’ SFRs. Engaging with the proxy cameras, we synthetic data pairs, which encode the optical aberrations and the random manufacturing biases, for training the learning-based algorithms. In correcting aberration, although promising results have been shown recently with convolutional neural networks, they are hard to generalize to stochastic machining biases. Therefore, we propose a dilated Omni-dimensional dynamic convolution (DOConv) and implement it in post-processing to account for the manufacturing degradation. Extensive experiments which evaluate multiple samples of two representative devices demonstrate that the proposed optimization framework accurately constructs the proxy camera. And the dynamic processing model is well-adapted to manufacturing deviations of different cameras, realizing perfect computational photography. The evaluation shows that the proposed method bridges the gap between optical design, system machining, and post-processing pipeline, shedding light on the joint of image signal reception (lens and sensor) and image signal processing (ISP).</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Chen_2022_TPAMI</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chen, Shiqi and Lin, Ting and Feng, Huajun and Xu, Zhihai and Li, Qi and Chen, Yueting}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Pattern Analysis and Machine Intelligence}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Computational Optics for Mobile Terminals in Mass Production}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{45}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4245-4259}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/chen_iccv-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/chen_iccv-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/chen_iccv-1400.webp"></source> <img src="/assets/img/publication_preview/chen_iccv.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="chen_iccv.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Chen_2021_ICCV" class="col-sm-8"> <div class="title">Extreme-Quality Computational Imaging via Degradation Framework</div> <div class="author"> <em>Shiqi Chen</em>, Huajun Feng, Keming Gao, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Zhihai Xu, Yueting Chen' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</em>, Oct 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/%5Biccv%5D%20Extreme-Quality%20Computational%20Imaging%20via%20Degradation%20Framework.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/TanGeeGo/DegradationTransfer" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>To meet the space limitation of optical elements, free-form surfaces or high-order aspherical lenses are adopted in mobile cameras to compress volume. However, the application of free-form surfaces also introduces the problem of image quality mutation. Existing model-based deconvolution methods are inefficient in dealing with the degradation that shows a wide range of spatial variants over regions. And the deep learning techniques in low-level and physics-based vision suffer from a lack of accurate data. To address this issue, we develop a degradation framework to estimate the spatially variant point spread functions (PSFs) of mobile cameras. When input extreme-quality digital images, the proposed framework generates degraded images sharing a common domain with real-world photographs. Supplied with the synthetic image pairs, we design a Field-Of-View shared kernel prediction network (FOV-KPN) to perform spatial-adaptive reconstruction on real degraded photos. Extensive experiments demonstrate that the proposed approach achieves extreme-quality computational imaging and outperforms the state-of-the-art methods. Furthermore, we illustrate that our technique can be integrated into existing postprocessing systems, resulting in significantly improved visual quality.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Chen_2021_ICCV</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chen, Shiqi and Feng, Huajun and Gao, Keming and Xu, Zhihai and Chen, Yueting}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Extreme-Quality Computational Imaging via Degradation Framework}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2632-2641}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/chen_tog-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/chen_tog-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/chen_tog-1400.webp"></source> <img src="/assets/img/publication_preview/chen_tog.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="chen_tog.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Chen_2021_TOG" class="col-sm-8"> <div class="title">Optical Aberrations Correction in Postprocessing Using Imaging Simulation</div> <div class="author"> <em>Shiqi Chen</em>, Huajun Feng, Dexin Pan, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Zhihai Xu, Qi Li, Yueting Chen' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>ACM Trans. Graph. (Post.Rec in SIGGRAPH 2022)</em>, Sep 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/%5Btog%5D%20Optical%20Aberrations%20Correction%20in%20Postprocessing%20Using%20Imaging%20Simulation.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/TanGeeGo/ImagingSimulation" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>As the popularity of mobile photography continues to grow, considerable effort is being invested in the reconstruction of degraded images. Due to the spatial variation in optical aberrations, which cannot be avoided during the lens design process, recent commercial cameras have shifted some of these correction tasks from optical design to postprocessing systems. However, without engaging with the optical parameters, these systems only achieve limited correction for aberrations.In this work, we propose a practical method for recovering the degradation caused by optical aberrations. Specifically, we establish an imaging simulation system based on our proposed optical point spread function model. Given the optical parameters of the camera, it generates the imaging results of these specific devices. To perform the restoration, we design a spatial-adaptive network model on synthetic data pairs generated by the imaging simulation system, eliminating the overhead of capturing training data by a large amount of shooting and registration.Moreover, we comprehensively evaluate the proposed method in simulations and experimentally with a customized digital-single-lens-reflex camera lens and HUAWEI HONOR 20, respectively. The experiments demonstrate that our solution successfully removes spatially variant blur and color dispersion. When compared with the state-of-the-art deblur methods, the proposed approach achieves better results with a lower computational overhead. Moreover, the reconstruction technique does not introduce artificial texture and is convenient to transfer to current commercial cameras.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Chen_2021_TOG</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chen, Shiqi and Feng, Huajun and Pan, Dexin and Xu, Zhihai and Li, Qi and Chen, Yueting}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Optical Aberrations Correction in Postprocessing Using Imaging Simulation}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">issue_date</span> <span class="p">=</span> <span class="s">{October 2021}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{40}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{5}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0730-0301}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3474088}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ACM Trans. Graph. (Post.Rec in SIGGRAPH 2022)}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{192}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{15}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{imaging simulation, deep-learning networks, image reconstruction, Optical aberrations}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%63%68%65%6E%73%71%30%31%32%30@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a> <a href="https://orcid.org/0000-0001-6396-1595" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=gJCsz90AAAAJ&amp;hl" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/TanGeeGo" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="/feed.xml" title="RSS Feed"><i class="fas fa-rss-square"></i></a> </div> <div class="contact-note"> You can even add a little note about which of these is the best way to reach you. </div> </div> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2024 Shiqi Chen. Last updated: February 17, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>