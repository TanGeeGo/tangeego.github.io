<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Degradation Transfer | Shiqi Chen</title> <meta name="author" content="Shiqi Chen"> <meta name="description" content="if the degradation on one image can transfer to another picture?"> <meta name="keywords" content="computational photography, optics, deep-learning, artificial intelligence"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%93%B8&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://tangeego.github.io/projects/2_project/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Shiqi </span>Chen</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/talks/">talks</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Degradation Transfer</h1> <p class="post-description">if the degradation on one image can transfer to another picture?</p> </header> <article> <div class="row"> <div class="col-sm d-flex justify-content-center mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/degradation%20calibration.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/degradation%20calibration.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/degradation%20calibration.gif-1400.webp"></source> <img src="/assets/img/publication_preview/degradation%20calibration.gif" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="abstract">Abstract:</h2> <p>To meet the space limitation of optical elements, free-form surfaces or high-order aspherical lenses are adopted in mobile cameras to compress volume. However, the application of free-form surfaces also introduces the problem of image quality mutation. Existing model-based deconvolution methods are inefficient in dealing with the degradation that shows a wide range of spatial variants over regions. And the deep learning techniques in low-level and physics-based vision suffer from a lack of accurate data. To address this issue, we develop a degradation framework to estimate the spatially variant point spread functions (PSFs) of mobile cameras. When input extreme-quality digital images, the proposed framework generates degraded images sharing a common domain with real-world photographs. Supplied with the synthetic image pairs, we design a Field-Of-View shared kernel prediction network (FOV-KPN) to perform spatial-adaptive reconstruction on real degraded photos. Extensive experiments demonstrate that the proposed approach achieves extreme-quality computational imaging and outperforms the state-of-the-art methods. Furthermore, we illustrate that our technique can be integrated into existing postprocessing systems, resulting in significantly improved visual quality.</p> <div class="row"> <div class="col-sm d-flex justify-content-center mt-3 mt-md-0"> <div style="max-width: 50%;"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/teaserfigure_iccv-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/teaserfigure_iccv-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/teaserfigure_iccv-1400.webp"></source> <img src="/assets/img/teaserfigure_iccv.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <div class="caption"> The reconstructions of a real-world image captured by HUAWEI HONOR 20. </div> <h2 id="background">Background:</h2> <p>The image quality always change greatly in the edge of FoV, which is generally caused by the manufacturing error. Directly measuring these deviations is really difficult after the lens is mounted, so we consider transfer the degradation of one image to another uncorrupted photo. Below, you will see the image mutation in the edge of mobile phone camera.</p> <div class="row"> <div class="col-sm d-flex justify-content-center mt-3 mt-md-0"> <div style="max-width: 60%;"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/background_iccv-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/background_iccv-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/background_iccv-1400.webp"></source> <img src="/assets/img/background_iccv.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <div class="caption"> Image quality mutation of the mobile camera. One can see that in the magnified region (right), the two sides separated by the dotted line have a significant difference in blurring. </div> <h2 id="method">Method:</h2> <p>Our aim is to transfer the degradation of one image to another. To achieve this,</p> <ul> <li>we built the first deep-learning-based calibration to densely represent the optical degradation of deviated camera.</li> </ul> <div class="row"> <div class="col-sm d-flex justify-content-center mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/method_iccv-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/method_iccv-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/method_iccv-1400.webp"></source> <img src="/assets/img/method_iccv.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Overview of the degradation framework. In stage I, backward transfer constructs the data pairs for the training procedure of degradation transfer. And in stage II, each degradation transfer is tailored to one FOV, aiming at estimating the spatially variant PSFs of different FOVs and generating realistic imaging results. </div> <p>The degradation transfer used for representation is controlled by many constraints:</p> \[\mathcal{L} = \alpha \mathcal{L}_{fidelity} + \beta \mathcal{L}_{sum2one} + \gamma \mathcal{L}_{boundary} + \delta \mathcal{L}_{sym}\] <table> <tbody> <tr> <td>here \(\alpha\), \(\beta\), \(\gamma\), \(\delta\) are the weight coefficients of different loss functions. $$\mathcal{L}_{fidelity}=</td> <td> </td> <td>x^{d}-y</td> <td> </td> <td> <em>{1}\(supervises the fidelity of the degraded result\)x^{d}\(.\)\mathcal{L}</em>{sum2one}=</td> <td>1-\Sigma k_{i,j}</td> <td>\(and\)\mathcal{L}_{boundary}=\Sigma</td> <td>k_{i,j} \cdot m_{i,j}</td> <td>\(are the loss functions on calculated point spread function\)k$$ (PSF).</td> </tr> </tbody> </table> \[\mathcal{L}_{sym} = \Sigma_{i,j}(k(i,j)-\frac{1}{2}(k(i,j)+k_{sym}(-i, -j)))^{2}\] <p>in this way, we force each PSF to be similar to its central symmetry counterpart. The unprocess and process pipeline is the same as the <a href="https://tangeego.github.io/projects/1_project/">Imaging Simulation</a></p> <p>After training the degradation transfer model, we could use it to copy the behavior of degradation and to influence the clean image. The evaluations on natural photographs are shown as follows</p> <div class="row"> <div class="col-sm d-flex justify-content-center mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/transfer_iccv-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/transfer_iccv-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/transfer_iccv-1400.webp"></source> <img src="/assets/img/transfer_iccv.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Comparison of real photographs and simulation results. (a) is the warpped digital images, (b) is the simulation result of our framework when input (a), and (c) is the real photograph. </div> <p>We construct a large dataset for each camera, and propose two deep learning models for correction. One model is blind and another is non-blind.</p> <div class="row"> <div class="col-sm d-flex justify-content-center mt-3 mt-md-0"> <div style="max-width: 80%;"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/network1_iccv-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/network1_iccv-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/network1_iccv-1400.webp"></source> <img src="/assets/img/network1_iccv.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <div class="caption"> The UNet-based network architecture is shown on the top, and the layer configurations illustrate with different colored blocks (bottom left). The FOV Block, Deformable ResBlock, and KPN Block are detailed in the bottom right corner. </div> <div class="row"> <div class="col-sm d-flex justify-content-center mt-3 mt-md-0"> <div style="max-width: 80%;"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/network2_iccv-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/network2_iccv-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/network2_iccv-1400.webp"></source> <img src="/assets/img/network2_iccv.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <div class="caption"> This model work like a deep weiner filter, which aims to perform frequency self-adaptive restoration on the input data. It engage with the predetermined PSFs to deconvolution and predict the targeted 1/SNR map. </div> <p>the key to perform deconvolution in latent space could be formulated as,</p> \[I_{latent}(x,y)=\Sigma_{i}^{m} \mathcal{F}^{-1}(\eta_{i} \cdot \frac{conj(\mathcal{F}(k_{i}))}{|\mathcal{F}(k_{i})|^{2}+\Sigma_{j}^{n} \phi_{ij} \cdot |\mathcal{F}(p_{j})|^{2}} \cdot \mathcal{F}(I_{degraded}(x,y)))\] <h2 id="experiments">Experiments:</h2> <p>We show some visualization here, for detailed comparisons and illustrations, please refer to our paper.</p> <div class="row"> <div class="col-sm d-flex justify-content-center mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/comparisons_iccv-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/comparisons_iccv-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/comparisons_iccv-1400.webp"></source> <img src="/assets/img/comparisons_iccv.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm d-flex justify-content-center mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/isp_iccv-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/isp_iccv-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/isp_iccv-1400.webp"></source> <img src="/assets/img/isp_iccv.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Real image restoration. The results of different methods magnify in the right and the positions highlight on the left. </div> <h2 id="our-main-observation">Our Main Observation:</h2> <ul> <li>The degradation of one image could be <strong>transferred</strong>. In this way, we could realize more accurate data-pair generation. As mentioned before, we constructed a degradation framework for estimating the spatially variant PSF of a specific camera, including but not limited to the device that shows image quality mutation. The proposed framework generates authentic imaging results that resemble real-world photographs, where a lot of shooting, registration, and color correction are needless.</li> <li>The imaging quality of low-end mobile terminal has the potential to surpass high-end DSLR.</li> </ul> <h2 id="limitations">Limitations:</h2> <ul> <li>The representation only work for one specific camera. If you want to transplant the transfer to another device, retraining is needed.</li> <li>Overfitting will introduce some wried texture in some high resolution natural images.</li> </ul> </article> <h2>References</h2> <div class="publications"> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Frequency%20self-adaptive%20OE-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Frequency%20self-adaptive%20OE-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Frequency%20self-adaptive%20OE-1400.webp"></source> <img src="/assets/img/publication_preview/Frequency%20self-adaptive%20OE.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Frequency self-adaptive OE.jpg" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Lin_2022_OE" class="col-sm-8"> <div class="title">Non-blind optical degradation correction via frequency self-adaptive and finetune tactics</div> <div class="author"> Ting Lin, <em>Shiqi Chen*</em>, Huajun Feng, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Zhihai Xu, Qi Li, Yueting Chen' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>Opt. Express</em>, Jun 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/%5Boe%5D%20Frequency%20self-adaptive.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/TanGeeGo/FSANet" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>In mobile photography applications, limited volume constraints the diversity of optical design. In addition to the narrow space, the deviations introduced in mass production cause random bias to the real camera. In consequence, these factors introduce spatially varying aberration and stochastic degradation into the physical formation of an image. Many existing methods obtain excellent performance on one specific device but are not able to quickly adapt to mass production. To address this issue, we propose a frequency self-adaptive model to restore realistic features of the latent image. The restoration is mainly performed in the Fourier domain and two attention mechanisms are introduced to match the feature between Fourier and spatial domain. Our method applies a lightweight network, without requiring modification when the fields of view (FoV) changes. Considering the manufacturing deviations of a specific camera, we first pre-train a simulation-based model, then finetune it with additional manufacturing error, which greatly decreases the time and computational overhead consumption in implementation. Extensive results verify the promising applications of our technique for being integrated with the existing post-processing systems.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Lin_2022_OE</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lin, Ting and Chen*, Shiqi and Feng, Huajun and Xu, Zhihai and Li, Qi and Chen, Yueting}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Opt. Express}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{All optical devices; Blind deconvolution; Image processing; Image quality; Optical design; Ray tracing}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{13}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{23485--23498}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Optica Publishing Group}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Non-blind optical degradation correction via frequency self-adaptive and finetune tactics}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{30}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/chen_iccv-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/chen_iccv-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/chen_iccv-1400.webp"></source> <img src="/assets/img/publication_preview/chen_iccv.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="chen_iccv.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Chen_2021_ICCV" class="col-sm-8"> <div class="title">Extreme-Quality Computational Imaging via Degradation Framework</div> <div class="author"> <em>Shiqi Chen</em>, Huajun Feng, Keming Gao, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Zhihai Xu, Yueting Chen' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</em>, Oct 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/%5Biccv%5D%20Extreme-Quality%20Computational%20Imaging%20via%20Degradation%20Framework.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/TanGeeGo/DegradationTransfer" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>To meet the space limitation of optical elements, free-form surfaces or high-order aspherical lenses are adopted in mobile cameras to compress volume. However, the application of free-form surfaces also introduces the problem of image quality mutation. Existing model-based deconvolution methods are inefficient in dealing with the degradation that shows a wide range of spatial variants over regions. And the deep learning techniques in low-level and physics-based vision suffer from a lack of accurate data. To address this issue, we develop a degradation framework to estimate the spatially variant point spread functions (PSFs) of mobile cameras. When input extreme-quality digital images, the proposed framework generates degraded images sharing a common domain with real-world photographs. Supplied with the synthetic image pairs, we design a Field-Of-View shared kernel prediction network (FOV-KPN) to perform spatial-adaptive reconstruction on real degraded photos. Extensive experiments demonstrate that the proposed approach achieves extreme-quality computational imaging and outperforms the state-of-the-art methods. Furthermore, we illustrate that our technique can be integrated into existing postprocessing systems, resulting in significantly improved visual quality.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Chen_2021_ICCV</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chen, Shiqi and Feng, Huajun and Gao, Keming and Xu, Zhihai and Chen, Yueting}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Extreme-Quality Computational Imaging via Degradation Framework}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2632-2641}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2024 Shiqi Chen. Last updated: February 17, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>