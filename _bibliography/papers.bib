---
---

@misc{jin2023reliable,
      title={Reliable Image Dehazing by NeRF}, 
      author={Zheyan Jin and Shiqi Chen and Huajun Feng and Zhihai Xu and Qi Li and Yueting Chen},
      year={2023},
      eprint={2303.09153},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{jin2023let,
      title={Let Segment Anything Help Image Dehaze}, 
      author={Zheyan Jin and Shiqi Chen and Yueting Chen and Zhihai Xu and Huajun Feng},
      year={2023},
      eprint={2306.15870},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{jin2023real,
      title={Toward Real Flare Removal: A Comprehensive Pipeline and A New Benchmark}, 
      author={Zheyan Jin and Shiqi Chen and Huajun Feng and Zhihai Xu and Yueting Chen},
      year={2023},
      eprint={2306.15884},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@ARTICLE{Li_2023_TGRS,
  author={Menghao Li and Ziran Zhang and Shiqi Chen and Zhihai Xu and Qi Li and Huajun Feng and Yueting Chen},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Imaging Simulation and Learning-based Image Restoration for Remote Sensing Time Delay and Integration Cameras}, 
  year={2023},
  volume={},
  number={},
  pages={1-1},}

@ARTICLE{Chen_2022_TPAMI,
  author={Shiqi Chen and Ting Lin and Huajun Feng and Zhihai Xu and Qi Li and Yueting Chen},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Computational Optics for Mobile Terminals in Mass Production}, 
  year={2023},
  volume={45},
  number={4},
  pages={4245-4259},
  preview={tolerance.gif},
  pdf={[tpami] Computational Optics for Mobile Terminals in Mass Production.pdf},
  selected={true}
}

@InProceedings{Chen_2021_ICCV,
  author    = {Shiqi Chen and Huajun Feng and Keming Gao and Zhihai Xu and Yueting Chen},
  title     = {Extreme-Quality Computational Imaging via Degradation Framework},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  month     = {October},
  year      = {2021},
  pages     = {2632-2641},
  preview   = {degradation calibration.gif},
  pdf={[iccv] Extreme-Quality Computational Imaging via Degradation Framework.pdf},
  selected={true}
}

@article{Chen_2021_TOG,
  author = {Shiqi Chen and Huajun Feng and Dexin Pan and Zhihai Xu and Qi Li and Yueting Chen},
  title = {Optical Aberrations Correction in Postprocessing Using Imaging Simulation},
  year = {2021},
  issue_date = {October 2021},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {40},
  number = {5},
  issn = {0730-0301},
  url = {https://doi.org/10.1145/3474088},
  abstract = {As the popularity of mobile photography continues to grow, considerable effort is being invested in the reconstruction of degraded images. Due to the spatial variation in optical aberrations, which cannot be avoided during the lens design process, recent commercial cameras have shifted some of these correction tasks from optical design to postprocessing systems. However, without engaging with the optical parameters, these systems only achieve limited correction for aberrations.In this work, we propose a practical method for recovering the degradation caused by optical aberrations. Specifically, we establish an imaging simulation system based on our proposed optical point spread function model. Given the optical parameters of the camera, it generates the imaging results of these specific devices. To perform the restoration, we design a spatial-adaptive network model on synthetic data pairs generated by the imaging simulation system, eliminating the overhead of capturing training data by a large amount of shooting and registration.Moreover, we comprehensively evaluate the proposed method in simulations and experimentally with a customized digital-single-lens-reflex camera lens and HUAWEI HONOR 20, respectively. The experiments demonstrate that our solution successfully removes spatially variant blur and color dispersion. When compared with the state-of-the-art deblur methods, the proposed approach achieves better results with a lower computational overhead. Moreover, the reconstruction technique does not introduce artificial texture and is convenient to transfer to current commercial cameras. Project Page: .},
  journal = {ACM Trans. Graph. (Post.Rec in SIGGRAPH 2022)},
  month = {sep},
  articleno = {192},
  numpages = {15},
  keywords = {imaging simulation, deep-learning networks, image reconstruction, Optical aberrations},
  preview   = {imaging simulation.gif},
  pdf={[tog] Optical Aberrations Correction in Postprocessing Using Imaging Simulation.pdf},
  selected={true}
}

@article{Lin_2022_OE,
  author = {Ting Lin and Shiqi Chen and Huajun Feng and Zhihai Xu and Qi Li and Yueting Chen},
  journal = {Opt. Express},
  keywords = {All optical devices; Blind deconvolution; Image processing; Image quality; Optical design; Ray tracing},
  number = {13},
  pages = {23485--23498},
  publisher = {Optica Publishing Group},
  title = {Non-blind optical degradation correction via frequency self-adaptive and finetune tactics},
  volume = {30},
  month = {Jun},
  year = {2022},
  url = {https://opg.optica.org/oe/abstract.cfm?URI=oe-30-13-23485},
  abstract = {In mobile photography applications, limited volume constraints the diversity of optical design. In addition to the narrow space, the deviations introduced in mass production cause random bias to the real camera. In consequence, these factors introduce spatially varying aberration and stochastic degradation into the physical formation of an image. Many existing methods obtain excellent performance on one specific device but are not able to quickly adapt to mass production. To address this issue, we propose a frequency self-adaptive model to restore realistic features of the latent image. The restoration is mainly performed in the Fourier domain and two attention mechanisms are introduced to match the feature between Fourier and spatial domain. Our method applies a lightweight network, without requiring modification when the fields of view (FoV) changes. Considering the manufacturing deviations of a specific camera, we first pre-train a simulation-based model, then finetune it with additional manufacturing error, which greatly decreases the time and computational overhead consumption in implementation. Extensive results verify the promising applications of our technique for being integrated with the existing post-processing systems.},
  preview = {Frequency self-adaptive OE.jpg},
  pdf = {[oe] Frequency self-adaptive.pdf}
}

@article{Li_2022_NC,
  title = {SRDiff: Single image super-resolution with diffusion probabilistic models},
  journal = {Neurocomputing},
  volume = {479},
  pages = {47-59},
  year = {2022},
  issn = {0925-2312},
  url = {https://www.sciencedirect.com/science/article/pii/S0925231222000522},
  author = {Haoying Li and Yifan Yang and Shiqi Chen and Meng Chang and Huajun Feng and Zhihai Xu and Qi Li and Yueting Chen},
  keywords = {Single image super-resolution, Diffusion probabilistic model, Diverse results, Deep learning},
  abstract = {Single image super-resolution (SISR) aims to reconstruct high-resolution (HR) images from given low-resolution (LR) images. It is an ill-posed problem because one LR image corresponds to multiple HR images. Recently, learning-based SISR methods have greatly outperformed traditional methods. However, PSNR-oriented, GAN-driven and flow-based methods suffer from over-smoothing, mode collapse and large model footprint issues, respectively. To solve these problems, we propose a novel SISR diffusion probabilistic model (SRDiff), which is the first diffusion-based model for SISR. SRDiff is optimized with a variant of the variational bound on the data likelihood. Through a Markov chain, it can provide diverse and realistic super-resolution (SR) predictions by gradually transforming Gaussian noise into a super-resolution image conditioned on an LR input. In addition, we introduce residual prediction to the whole framework to speed up model convergence. Our extensive experiments on facial and general benchmarks (CelebA and DIV2K datasets) show that (1) SRDiff can generate diverse SR results with rich details and achieve competitive performance against other state-of-the-art methods, when given only one LR input; (2) SRDiff is easy to train with a small footprint(The word “footprint” in this paper represents “model size” (number of model parameters).); (3) SRDiff can perform flexible image manipulation operations, including latent space interpolation and content fusion.},
  preview = {SRDiff.jpg},
  pdf = {[neurocomputing] SRDiff.pdf}
}

@article{XU_2023_OLEN,
  title = {Hyperspectral image reconstruction based on the fusion of diffracted rotation blurred and clear images},
  journal = {Optics and Lasers in Engineering},
  volume = {160},
  pages = {107274},
  year = {2023},
  issn = {0143-8166},
  url = {https://www.sciencedirect.com/science/article/pii/S014381662200327X},
  author = {Hao Xu and Haiquan Hu and Shiqi Chen and Zhihai Xu and Qi Li and Tingting Jiang and Yueting Chen},
  keywords = {Hyperspectral image reconstruction, Hyperspectral imaging, Diffraction, Image fusion, Convolutional neural network},
  abstract = {To overcome the problems of imaging speed and bulky volume of the traditional hyperspectral imaging systems, the recently proposed compact, snapshot hyperspectral imaging system with diffracted rotation has attracted a lot of interest. Due to the severe degradation of the diffracted rotation blurred image, the restored hyperspectral image (HSI) suffers from a lack of spatial detail information and spectral accuracy. To improve the quality of the reconstructed HSI, we present a joint imaging system of diffractive imaging and clear imaging as well as a convolutional neural network (CNN) based method with two input branches for HSI reconstruction. In the reconstruction network, we develop a feature extraction block (FEB) to extract the features of the two input images, respectively. Subsequently, a double residual block (DRB) is designed to fuse and reconstruct the extracted features. Experimental results show that HSI with high spatial resolution and spectral accuracy can be reconstructed. Our method outperforms the state-of-the-art methods in terms of quantitative metrics and visual quality.},
  preview = {HSI.jpg},
  pdf = {[olen] HSI.pdf}
}

@article{Yu_2023_AO,
  author = {Fangzheng Yu and Nan Xu and Shiqi Chen and Huajun Feng and Zhihai Xu and Qi Li and Tingting Jiang and Yueting Chen},
  journal = {Appl. Opt.},
  keywords = {Augmented reality; Distortion; Image metrics; Image processing; Image resolution; Neural networks},
  number = {21},
  pages = {5720--5726},
  publisher = {Optica Publishing Group},
  title = {Direct distortion prediction method for AR-HUD dynamic distortion correction},
  volume = {62},
  month = {Jul},
  year = {2023},
  url = {https://opg.optica.org/ao/abstract.cfm?URI=ao-62-21-5720},
  abstract = {Dynamic distortion is one of the most critical factors affecting the experience of automotive augmented reality head-up displays (AR-HUDs). A wide range of views and the extensive display area result in extraordinarily complex distortions. Existing methods based on the neural network first obtain distorted images and then get the predistorted data for training mostly. This paper proposes a distortion prediction framework based on the neural network. It directly trains the network with the distorted data, realizing dynamic adaptation for AR-HUD distortion correction and avoiding errors in coordinate interpolation. Additionally, we predict the distortion offsets instead of the distortion coordinates and present a field of view (FOV)-weighted loss function based on the spatial-variance characteristic to further improve the prediction accuracy of distortion. Experiments show that our methods improve the prediction accuracy of AR-HUD dynamic distortion without increasing the network complexity or data processing overhead.},
  preview = {AR-HUD.png},
  pdf = {[ao] AR-HUD.pdf}
}

@article{Xu_2023_OE,
  author = {Nan Xu and Hao Xu and Shiqi Chen and Haiquan Hu and Zhihai Xu and Huajun Feng and Qi Li and Tingting Jiang and Yueting Chen},
  journal = {Opt. Express},
  keywords = {Diffractive optical elements; Hyperspectral imaging; Image processing; Image quality; Imaging systems; Wave optics},
  number = {12},
  pages = {20489--20504},
  publisher = {Optica Publishing Group},
  title = {Snapshot hyperspectral imaging based on equalization designed DOE},
  volume = {31},
  month = {Jun},
  year = {2023},
  url = {https://opg.optica.org/oe/abstract.cfm?URI=oe-31-12-20489},
  abstract = {Hyperspectral imaging attempts to determine distinctive information in spatial and spectral domain of a target. Over the past few years, hyperspectral imaging systems have developed towards lighter and faster. In phase-coded hyperspectral imaging systems, a better coding aperture design can improve the spectral accuracy relatively. Using wave optics, we post an equalization designed phase-coded aperture to achieve desired equalization point spread functions (PSFs) which provides richer features for subsequent image reconstruction. During the reconstruction of images, our raised hyperspectral reconstruction network, CAFormer, achieves better results than the state-of-the-art networks with less computation by substituting self-attention with channel-attention. Our work revolves around the equalization design of the phase-coded aperture and optimizes the imaging process from three aspects: hardware design, reconstruction algorithm, and PSF calibration. Our work is putting snapshot compact hyperspectral technology closer to a practical application.},
  preview = {equalization doe.png},
  pdf = {[oe] Equalization Doe.pdf}
}

@misc{Chen_2023_PRL,
  title={Mobile Image Restoration via Prior Quantization}, 
  author={Shiqi Chen and Jinwen Zhou and Menghao Li and Yueting Chen and Tingting Jiang},
  year={2023},
  eprint={2305.05899},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2305.05899},
  preview={prior quantization.png},
  pdf = {[prl] Mobile Image Restoration via Prior Quantization.pdf}
}

@inproceedings{zhou_2023_IMT,
  title={DR-UNet: dynamic residual U-Net for blind correction of optical degradation},
  author={Jinwen Zhou and Shiqi Chen and Qi Li and Tongyue Li and Huajun Feng},
  booktitle={Conference on Infrared, Millimeter, Terahertz Waves and Applications (IMT2022)},
  volume={12565},
  pages={326--338},
  year={2023},
  organization={SPIE}
}

@Article{Yang_2022_Electronic,
  AUTHOR = {Jiaqi Yang and Shiqi Chen and Qi Li and Tingting Jiang and Yueting Chen and Jing Wang},
  TITLE = {Epistemic-Uncertainty-Based Divide-and-Conquer Network for Single-Image Super-Resolution},
  JOURNAL = {Electronics},
  VOLUME = {11},
  YEAR = {2022},
  NUMBER = {22},
  ARTICLE-NUMBER = {3809},
  URL = {https://www.mdpi.com/2079-9292/11/22/3809},
  ISSN = {2079-9292},
  ABSTRACT = {The introduction of convolutional neural networks (CNNs) into single-image super-resolution (SISR) has resulted in remarkable performance in the last decade. There is a contradiction in SISR between indiscriminate processing and the different processing difficulties in different regions, leading to the need for locally differentiated processing of SR networks. In this paper, we propose an epistemic-uncertainty-based divide-and-conquer network (EU-DC) in order to address this problem. Firstly, we build an image-gradient-based divide-and-conquer network (IG-DC) that utilizes gradient-based division to separate degraded images into easy and hard processing regions. Secondly, we model the IG-DC&rsquo;s epistemic uncertainty map (EUM) by using Monte Carlo dropout and, thus, measure the output confidence of the IG-DC. The lower the output confidence is, the more difficult the IG-DC is to process. The EUM-based division is generated by quantizing the EUM into two levels. Finally, the IG-DC is transformed into an EU-DC by substituting the gradient-based division with EUM-based division. Our extensive experiments demonstrate that the proposed EU-DC achieves better reconstruction performance than that of multiple state-of-the-art SISR methods in terms of both quantitative and visual quality.},
  pdf = {[ele] ep-uncertainty super-resolution.pdf},
  preview = {ep-uncertainty super-resolution.jpg}
}
